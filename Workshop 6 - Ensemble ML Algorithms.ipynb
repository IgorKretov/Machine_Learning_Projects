{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Machine Learning Algorithms in Python with scikit-learn\n",
    "\n",
    "Ensembles can give you a boost in accuracy on your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Model Predictions Into Ensemble Predictions\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "1. Bagging. Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "\n",
    "2. Boosting. Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain.\n",
    "\n",
    "3. Voting. Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions.\n",
    "This post will not explain each of these methods.\n",
    "\n",
    "It assumes you are generally familiar with machine learning algorithms and ensemble methods and that you are looking for information on how to create ensembles in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors:\n",
      "[['6' 148.0 72.0 35.0 0.0 33.6 0.627 50.0]\n",
      " ['1' 85.0 66.0 29.0 0.0 26.6 0.35100000000000003 31.0]\n",
      " ['8' 183.0 64.0 0.0 0.0 23.3 0.672 32.0]\n",
      " ['1' 89.0 66.0 23.0 94.0 28.1 0.16699999999999998 21.0]\n",
      " ['0' 137.0 40.0 35.0 168.0 43.1 2.2880000000000003 33.0]]\n",
      "Targets:\n",
      "[1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "url = \"https://gist.githubusercontent.com/ktisha/c21e73a1bd1700294ef790c56c8aec1f/raw/819b69b5736821ccee93d05b51de0510bea00294/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[9:, 0:8]\n",
    "Y = array[9:, 8]\n",
    "Y = Y.astype('int')\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "print(\"Predictors:\")\n",
    "print(X[:5])\n",
    "print(\"Targets:\")\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Algorithms\n",
    "### 1. Bagged Decision Tree\n",
    "Bagging perform best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770745044429255\n"
     ]
    }
   ],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest\n",
    "Random forest is an extension of bagged decision tress.\n",
    "\n",
    "Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individul classifers. Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features are considered for each split.\n",
    "\n",
    "You can construct a Random Forest model for classification using the RandomForestClassifier class.\n",
    "\n",
    "The example below provides an example of Random Forest for classification with 100 trees and split points chosen from a random selection of 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7655844155844156\n"
     ]
    }
   ],
   "source": [
    "# Random Forest classification\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extra Trees\n",
    "Extra Trees are another modification of bagging where random trees are cnstructed from samples of the training dataset.\n",
    "\n",
    "You can constructed an Entra Trees model for classification using the ExtraTreesClassifier class.\n",
    "\n",
    "The example below provides ademonstration of extra trees with the number of trees set to 100 and splits chosen from 7 random feartures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7629186602870812\n"
     ]
    }
   ],
   "source": [
    "#Extra Trees Classification\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Algorithms\n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence.\n",
    "\n",
    "Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are conbined to create a final output prediction.\n",
    "\n",
    "The two most common boosting ensemble machine learning algorithms are:\n",
    "\n",
    "1. AdaBoost\n",
    "2. Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. AdaBoost\n",
    "Adaboost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instance in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or less attention to them in the construction of subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760457963089542\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "num_trees = 30\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stochastic Gradient Boosting Classification\n",
    "Stochastic Gradient Boosting are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps of the best techniques available for improving performance via ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7733253588516746\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Ensemble\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n",
    "\n",
    "It works by first creating two or more standalone models from training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n",
    "\n",
    "The predictions of the sub-models can be weighted, but specifying the weights for classifier manually or even heirostically is difficult. More advanced methods can learn how to best weight the predictions from submodels, but this is called stacking(stacked aggregation) and is currently not provided in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Elizabeth.Ke.Zhang/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/Elizabeth.Ke.Zhang/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/Elizabeth.Ke.Zhang/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/Elizabeth.Ke.Zhang/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/Elizabeth.Ke.Zhang/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/Elizabeth.Ke.Zhang/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/Elizabeth.Ke.Zhang/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7304169514695831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Elizabeth.Ke.Zhang/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/Elizabeth.Ke.Zhang/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/Elizabeth.Ke.Zhang/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC  # C-Support Vector Classification.\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# create the ssub models\n",
    "estimators= []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this post you discovered ensemble machine learning algorithms for improving the performance of models on your problems.\n",
    "\n",
    "You learn about:\n",
    "\n",
    "1. Bagging Ensembles including Bagged Decision Trees, Random Forest and Extra Trees.\n",
    "2. Boosting Ensembles including AdaBoost and Stochastic Gradient Boosting.\n",
    "3. Voting Ensembles for averaging te predictions for any arbitrary models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
