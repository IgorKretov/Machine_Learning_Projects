{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "1.Simple intuition behind Neural networks.\n",
    "\n",
    "2.Multi-Layer Perception and its basics.\n",
    "\n",
    "3.Steps involved in Neural Network methodology.\n",
    "\n",
    "4.Visualizing steps for Neural Network working methdology.\n",
    "\n",
    "5.Implementing NN using Numpy(Python).\n",
    "\n",
    "6.Implementing NN using R.\n",
    "\n",
    "7.\\[Optional\\]Mathematical Perspective of Back Propagration Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple intuition behind neural networks\n",
    "\n",
    "Neural Networks work in very similar manner. It takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer. This result estimation process is technically known as \"Forward Propagation\"\n",
    "\n",
    "Next, we compare the result with actual output. The task is to make the output to neural network as close to actual(desired) output. Each of these neurons are contributing some error to final output. How do you reduce the error?\n",
    "\n",
    "We try to minimize the value/weight of neurons those are contributing more to the error and this happens while traveling back to the neurons of the neural network and finding where the error lies. This process is known as \"Backward Progation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perception and its basics \n",
    "About Gradient Descent: \n",
    "https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps involved in Neural Network methodology.\n",
    "\n",
    "0). We take input and output X and Y.\n",
    "\n",
    "1). We initialize weights and biases with random values (This is one time initiation. In the next iteration, we will use updated weights, and biases). \n",
    "\n",
    "2). We take matrix dot product of input and weights assigned to edges between the input and hidden layer then add biases of the hidden layer neurons to trspective inputs, this is konown as linear transformation:\n",
    "\n",
    "hidden_layer_input = matrix_dot_product(X, wh) + bh\n",
    "\n",
    "3). Perform non-linear transformation using an activation function(Sigmoid). Sigmoid will return the output as 1/(1+exp(-x)).\n",
    "\n",
    "hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "\n",
    "4). Perform a linear transformation on hidden layer activation(take matrix dot product with weights and add a bias of the output layer neuron) then apply an activation function (again used sigmoid, but you can use any other activation function depending upon your task) to predict the output.\n",
    "\n",
    "output_layer_input = matrix_dot_product(hiddenlayer_activations * wout)+bout\n",
    "\n",
    "output = sigmoid(output_layer_input)\n",
    "\n",
    "### All above steps are known as \"Forward Propagation\"\n",
    "\n",
    "5).Compare prediction with actual output and calculate the gradient of error (Actual – Predicted). Error is the mean square loss = ((Y-t)^2)/2\n",
    "\n",
    "E = y – output\n",
    "\n",
    "6.) Compute the slope/ gradient of hidden and output layer neurons ( To compute the slope, we calculate the derivatives of non-linear activations x at each layer for each neuron). Gradient of sigmoid can be returned as x * (1 – x).\n",
    "\n",
    "slope_output_layer = derivatives_sigmoid(output) \n",
    "\n",
    "slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "\n",
    "7.) Compute change factor(delta) at output layer, dependent on the gradient of error multiplied by the slope of output layer activation\n",
    "\n",
    "d_output = E * slope_output_layer\n",
    "\n",
    "8.) At this step, the error will propagate back into the network which means error at hidden layer. For this, we will take the dot product of output layer delta with weight parameters of edges between the hidden and output layer (wout.T).\n",
    "\n",
    "Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose) \n",
    "\n",
    "9.)Compute change factor(delta) at hidden layer, multiply the error at hidden layer with slope of hidden layer activation \n",
    "\n",
    "d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "\n",
    "10.) Update weights at the output and hidden layer: The weights in the network can be updated from the errors calculated for training example(s).\n",
    "\n",
    "wout = wout + matrix_dot_product(\n",
    "hiddenlayer_activations.Transpose, d_output)*learning_rate \n",
    "\n",
    "wh = wh + matrix_dot_product(\n",
    "X.Transpose,d_hiddenlayer)*learning_rate \n",
    "\n",
    "learning_rate: The amount that weights are updated is controlled by a configuration parameter called the learning rate) \n",
    "\n",
    "11.) Update biases at the output and hidden layer: The biases in the network can be updated from the aggregated errors at that neuron. \n",
    "\n",
    "• bias at output_layer =bias at output_layer + sum of delta of output_layer at row-wise * learning_rate \n",
    "\n",
    "• bias at hidden_layer =bias at hidden_layer + sum of delta of output_layer at row-wise * learning_rate \n",
    "\n",
    "bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate \n",
    "\n",
    "bout = bout + sum(d_output, axis=0)*learning_rate\n",
    "\n",
    "### Steps from 5 to 11 are known as \"Backward Propagation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing NN using Numpy(Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input array\n",
    "X = np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "# Output\n",
    "y = np.array([[1],[1],[0]])\n",
    "\n",
    "# Sigmoid Function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98143605]\n",
      " [0.96569667]\n",
      " [0.04737666]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Variable initialization\n",
    "epoch=5000 #Setting training iterations\n",
    "learning_rate = 0.1\n",
    "inputlayer_neurons = X.shape[1]\n",
    "hiddenlayer_neurons = 3\n",
    "output_neurons = 1\n",
    "\n",
    "# weight and bias initialization\n",
    "wh = np.random.uniform(size=(inputlayer_neurons, hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons)) \n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons)) \n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "for i in range(epoch):\n",
    "    # Forward Propogation \n",
    "    hidden_layer_input1=np.dot(X,wh) + bh\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input=np.dot(hiddenlayer_activations,wout) + bout \n",
    "    output = sigmoid(output_layer_input)\n",
    "    \n",
    "    # Backpropagation\n",
    "    E = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    d_output = E * slope_output_layer\n",
    "    Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += hiddenlayer_activations.T.dot(d_output) *learning_rate\n",
    "    bout += np.sum(d_output, axis=0,keepdims=True) *learning_rate\n",
    "    wh += X.T.dot(d_hiddenlayer) *learning_rate\n",
    "    bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *learning_rate\n",
    "    \n",
    "print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
